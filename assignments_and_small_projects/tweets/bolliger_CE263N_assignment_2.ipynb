{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Shapely is a python package that relies on the same geometry engine (GEOS) as does PostGIS. It is designed for shorter-term analysis, when the management of a large relational database is not part of your intentions. It is often quicker to use for ad-hoc spatial analyses, does not depend on the (somtimes complicated) creation of the highly structured RDBMS, and relies on python idioms that can be more familiar and easier to work with than those of the SQL/GIS world. It handles operations on *geometries* (points, curves, and surfaces), but not *geographies* (in the PostGIS data type sense). It would not be suitable for spatial calculations in which your data is highly spatially distributed and you need to take into account the spheroid shape of the Earth. Since it relies on the same C++ engine (GEOS), I expect the performance to be similar to that of PostGIS. However, the documentation notes that there is some overhead associated with creating geometries involving many coordinates. There is a \"shapely.speedups\" module that seems to help with this, but very little information on how this module is implemented.\n",
    "\n",
    "PostGIS, on the other hand, is a RDBMS. It relies on SQL to query data and offers the same geometric manipulation tools that Shapely does. In addition, it allows the possibility of associating your data with spherical (lat/long) coordinates. While shapely works entirely in cartesian coordinate systems and does not involve any geographic projections, PostGIS allows more rigorous geospatial analysis across the globe. It is the better choice if you are managing a database over a longer timeframe and will need to query it multiple times in numerous different ways. It is also a better choice if your data is highly spatially distributed (because of it's spherical representation capabilities). It is likely also faster for spatial relational queries of large size due to spatial indexing of the database. Because it is often time-consuming to set up and requires a specific data format, it is likely not the best choice for short, ad-hoc spatial analyses.\n",
    "\n",
    "Fiona is a python package used for reading and writing data files, and it is often used in combination with a tool like shapely. It is useful when you want to turn an arbitrary data type (for instance a CSV with lat/long columns) into spatial data and to convert between spatial data formats.\n",
    "\n",
    "**In summary**: For ad-hoc spatial analysis of relatively small datasets, the Fiona/Shapely combination is probably the easiest way to go. For longer-term projects with spatial data that will require database management over time and/or analysis of large amounts of data across large geographic scales, inserting your data into a PostGIS database and relying on SQL queries is probably the way to go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports and settings\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from os.path import join\n",
    "import csv\n",
    "from IPython.display import Image,Latex\n",
    "from sklearn.cluster import KMeans,DBSCAN\n",
    "from pyproj import Proj\n",
    "import geojson\n",
    "import geopandas\n",
    "import shapely as shp\n",
    "\n",
    "# Turn off annoything SettingWithCopy warnings\n",
    "pd.set_option('mode.chained_assignment',None)\n",
    "\n",
    "# SRID for geographic CS (for importing from lat/long)\n",
    "SRID_geog = 4326\n",
    "\n",
    "# SRID for projection CS (UTM Zone in Northern Cal)\n",
    "SRID_proj = 32610\n",
    "\n",
    "# random seed\n",
    "seed = 1234\n",
    "\n",
    "# set filepaths\n",
    "data_loc = u\"\"\"/Users/ianbolliger/Box Sync/grad_school/courses/2015_fall\\\n",
    "/spatial_analytics/assignments/assignment_2\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify working PostGIS and spatially-enabled database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"dbname=template_postgis host=localhost\")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"SELECT srtext FROM spatial_ref_sys WHERE srid = 32610\"\"\")\n",
    "rows = cur.fetchall()\n",
    "print rows[0]\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1) Create database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "> psql\n",
    "$ CREATE DATABASE assignment_2 OWNER bolliger TEMPLATE template_postgis ENCODING 'utf8';\n",
    "$ \\connect assignment_2\n",
    "$ CREATE TABLE tweets (\n",
    "        id bigint,\n",
    "        userid integer,\n",
    "        loc geometry,\n",
    "        time timestamptz,\n",
    "        text text);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)  Parse tweets coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "tweets = pd.read_json(join(data_loc,'tweets_1M.json'))\n",
    "\n",
    "# select 100k subset of tweets\n",
    "n_subset = 100000\n",
    "subset = tweets.sample(n=n_subset, random_state=seed)\n",
    "\n",
    "# parse dates\n",
    "subset['time'] = pd.to_datetime(subset['timeStamp'])\n",
    "\n",
    "# fix lat/lng format\n",
    "subset['loc']=\"SRID=\" + str(SRID_geog) + \";POINT(\" + subset['lng'].astype('string') + ' ' + subset['lat'].astype('string') + \")\"\n",
    "\n",
    "# keep necessary columns\n",
    "subset=subset.reindex(columns=['id','user_id','loc','time','text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Insert 100K tweets into PostGIS table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# connect to database\n",
    "conn = psycopg2.connect(\"dbname=assignment_2 host=localhost\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save as CSV for COPY FROM\n",
    "subset.to_csv(join(data_loc,'tweets.csv'),index=False, encoding='utf8',quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "# COPY to postGIS table (could only figure out how to copy if SRID=4326)\n",
    "cur.execute(\"COPY tweets FROM %s DELIMITER ',' CSV HEADER\", (join(data_loc,\"tweets.csv\"),))\n",
    "\n",
    "# Transform to UTM projection (needed for distance calcs in meters)\n",
    "cur.execute(\"\"\"ALTER TABLE tweets\n",
    "                ALTER COLUMN loc TYPE geometry(POINT,%s) USING ST_Transform(loc,%s)\"\"\", (SRID_proj,SRID_proj))\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5) Convert shapefile into suitable form for database and insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "> shp2pgsql -I -W \"latin1\" -s 4326:32610 tl_2010_06_county10.shp public.ca_census_tract | psql -d assignment_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Calculate number of tweets inside Contra Costa County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"SELECT COUNT(*) from tweets t,ca_census_tract ct\n",
    "    WHERE \n",
    "        ct.name10 = 'Contra Costa'\n",
    "    AND\n",
    "        ST_CONTAINS(ct.geom,t.loc)\"\"\")\n",
    "tweets_in_CoCo = cur.fetchone()\n",
    "print \"tweets from 100K subset that fall within Contra Costa County:\", tweets_in_CoCo[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Calculate number of tweets 100 miles outside Alameda County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 100 miles = 160934 meters\n",
    "\n",
    "cur.execute(\"\"\"SELECT COUNT(*) FROM tweets t, ca_census_tract ct\n",
    "    WHERE\n",
    "        ct.name10 = 'Alameda'\n",
    "    AND NOT\n",
    "        ST_DWITHIN(t.loc,ct.geom,160934)\"\"\")\n",
    "tweets_outside_AC = cur.fetchone()\n",
    "print \"tweets from 100K subset that fall 100 miles outside of Alameda County:\", tweets_outside_AC[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 8) Insert 2010 Census pop per county into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "> psql assignment_2\n",
    "$ CREATE TABLE ca_census_data\n",
    "    (GEOID varchar(11),\n",
    "    SUMLEV varchar(3),\n",
    "    STATE varchar(2),\n",
    "    COUNTY varchar(3),\n",
    "    CBSA varchar(5),\n",
    "    CSA varchar(3),\n",
    "    NECTA integer,\n",
    "    CNECTA integer,\n",
    "    NAME varchar(30),\n",
    "    POP100 integer,\n",
    "    HU100 integer,\n",
    "    POP1002000 integer,\n",
    "    HU1002000 integer,\n",
    "    P001001 integer,\n",
    "    P0010012000 integer);\n",
    "$ COPY ca_census_data FROM '/Users/ianbolliger/Box Sync/grad_school/courses/2015_fall/spatial_analytics/assignments/assignment_2' CSV HEADER;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Provide visualization of tweets per-capita "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"CREATE TABLE tweets_pc AS\n",
    "\n",
    "WITH n_tweets as (\n",
    "SELECT ct.name10,ct.countyfp10,COUNT(t.loc) as tweets_per_county,ct.geom\n",
    "    FROM tweets t\n",
    "    RIGHT JOIN ca_census_tract ct\n",
    "        ON ST_CONTAINS(ct.geom,t.loc)\n",
    "    GROUP BY ct.countyfp10,ct.name10,ct.geom)\n",
    "\n",
    "\n",
    "SELECT n_tweets.name10,n_tweets.tweets_per_county/cd.pop100::float as tweets_per_capita,n_tweets.geom\n",
    "    FROM n_tweets\n",
    "    INNER JOIN ca_census_data as cd\n",
    "        ON cd.county = n_tweets.countyfp10\"\"\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following visualization was constructed in QGIS from the \"tweets_pc\" table constructed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(join(data_loc,'tpc.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Find radii of DBSCAN tweet clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take 10k sample of tweets\n",
    "subset_10 = tweets.sample(n=10000, random_state=seed)\n",
    "\n",
    "# Project into UTM system\n",
    "myProj = Proj(\"+proj=utm +zone=10 +ellps=WGS84 +datum=WGS84\")\n",
    "UTMx,UTMy = myProj(subset_10['lng'].values,subset_10['lat'].values)\n",
    "subset_10['m_N'] = UTMy\n",
    "subset_10['m_E'] = UTMx\n",
    "coords = subset_10[['m_N','m_E']]\n",
    "\n",
    "# Run DBSCAN\n",
    "eps = 1000 #meters\n",
    "min_samples = 10\n",
    "\n",
    "db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "coords['labels'] = db.fit_predict(coords)\n",
    "\n",
    "# keep non-outliered points\n",
    "coords = coords[coords['labels'] != -1]\n",
    "\n",
    "# count number of non-outlier points\n",
    "n = np.count_nonzero(db.labels_ + 1)\n",
    "print \"non-outliers:\", n\n",
    "\n",
    "# count number of clusters\n",
    "n_clust = len(set(coords['labels']))\n",
    "print \"number of clusters:\", n_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find centroids\n",
    "grouped = coords.groupby('labels')\n",
    "centroids = grouped.mean()\n",
    "counts = grouped['m_N'].count()\n",
    "dists = coords.join(centroids,on='labels',rsuffix='_centroid').join(counts,on='labels',rsuffix='count')\n",
    "dists.rename(columns={'m_Ncount':'n_tweets'})\n",
    "\n",
    "# calc distance\n",
    "dists['dist'] = np.sqrt((dists['m_N']-dists['m_N_centroid'])**2 + (dists['m_E']-dists['m_E_centroid'])**2)\n",
    "dists.head()\n",
    "\n",
    "# calc min radius and total tweets for each cluster\n",
    "grouped = dists[['labels','dist']].groupby('labels')\n",
    "rad = grouped.max().rename(columns={'dist':'Minimum Radius (m)'})\n",
    "n_tweets = grouped.count().rename(columns={'dist':'# Tweets'})\n",
    "\n",
    "# show table\n",
    "tab = rad.join(n_tweets).sort('Minimum Radius (m)',ascending=False)\n",
    "tab.index.name = 'Cluster ID'\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Connect to MongoDB Instance and set up database and collection\n",
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "mongo_client = MongoClient()\n",
    "mongo_db = mongo_client.san_francisco_db\n",
    "tweets_col = mongo_db.tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## cluster using 2-step algorithm from HW1\n",
    "\n",
    "# Project into UTM system\n",
    "myProj = Proj(\"+proj=utm +zone=10 +ellps=WGS84 +datum=WGS84\")\n",
    "UTMx,UTMy = myProj(tweets['lng'].values,tweets['lat'].values)\n",
    "coords = pd.DataFrame({'m_E':UTMx,'m_N':UTMy})\n",
    "tweets['m_N'] = UTMy\n",
    "tweets['m_E'] = UTMx\n",
    "coords = tweets[['m_N','m_E']]\n",
    "\n",
    "# parameters\n",
    "k1 = 2\n",
    "e = 100 # 100 meter epsilon\n",
    "min_samples = 100\n",
    "\n",
    "# initialize algorithms object\n",
    "km = KMeans(init='k-means++', n_clusters=k1, random_state=seed)\n",
    "db = DBSCAN(eps=e, min_samples=min_samples)\n",
    "\n",
    "# copy coordinates so that we can assign clusters in new dataframe\n",
    "data = coords.copy()\n",
    "data['cluster2'] = np.nan\n",
    "\n",
    "# run step 1\n",
    "km.fit(coords)\n",
    "data.loc[:,'cluster1'] = pd.Series(km.labels_, index = data.index)\n",
    "    \n",
    "# run DBSCAN on each cluster\n",
    "for c in range(k1):\n",
    "    ss = coords[data['cluster1']==c]\n",
    "    db.fit(ss)\n",
    "    ss = pd.DataFrame({'cluster2':db.labels_}, index = ss.index)\n",
    "    data = data.combine_first(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge cluster data back into tweets dataframe\n",
    "non_outliers = data[data['cluster2'] != -1]\n",
    "non_outliers['cluster2']=non_outliers['cluster2'].astype('int')\n",
    "non_outliers['cluster_ID'] = non_outliers['cluster1'].astype('string') + \"-\" + non_outliers['cluster2'].astype('string')\n",
    "clustered_tweets = tweets[['user_id','timeStamp','text','lng','lat']].join(non_outliers['cluster_ID'],how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data into GeoDataFrame\n",
    "tweets_gdf = geopandas.GeoDataFrame(clustered_tweets[['timeStamp','text','cluster_ID']])\n",
    "tweets_gdf['geometry'] = geopandas.GeoSeries([shp.geometry.Point(x,y)for x,y in zip(clustered_tweets['lng'],clustered_tweets['lat'])],index=tweets_gdf.index)\n",
    "\n",
    "# convert timestamp to datetime (disabled b/c GeoJSON can't take timestamps)\n",
    "#tweets_gdf['timeStamp'] = pd.to_datetime(tweets_gdf['timeStamp'])\n",
    "\n",
    "# dumps to geoJSON\n",
    "tweets_gj = tweets_gdf.to_json()\n",
    "tweets_gj = geojson.loads(tweets_gj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Insert into DB\n",
    "for i in tweets_gj['features']:\n",
    "    tweets_db.insert(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
