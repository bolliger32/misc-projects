{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO for next round of submissions:\n",
    "- submit 5c (shuffled training) to compare performance with current submission (5b)\n",
    "- do hyperparameter optimization via grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factors that can influence taxi fare:\n",
    "- Distance\n",
    "- Travel time\n",
    "- Time of day\n",
    "- Airport start\n",
    "- Airport finish\n",
    "- West/southbound bridge crossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "import shapely as shp\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble.forest import RandomForestRegressor\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pyproj import Proj\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "import folium\n",
    "from IPython.display import HTML\n",
    "\n",
    "import json\n",
    "import urllib\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(fname,modified=True,end_dates=True,shuffle=True):\n",
    "    names=['ID', 'start_date', 'end_date', 'fare', 'passengers',\n",
    "            'start_lo', 'start_la', 'end_lo', 'end_la',\n",
    "            'start_TAZ', 'end_TAZ']\n",
    "    np.random.seed(seed)\n",
    "    if modified:\n",
    "        if end_dates:\n",
    "            data = pd.read_csv(\n",
    "                fname,\n",
    "                skiprows=1,\n",
    "                names=names+['dist','timecost','distcost'],\n",
    "                index_col=0,\n",
    "                parse_dates=['start_date','end_date'])\n",
    "        else:\n",
    "            data = pd.read_csv(\n",
    "                fname,\n",
    "                skiprows=1,\n",
    "                names=names+['dist','timecost','distcost'],\n",
    "                index_col=0,\n",
    "                parse_dates=['start_date'])\n",
    "            \n",
    "        if shuffle:\n",
    "            # shuffle indices\n",
    "            new_ix = list(data.index)\n",
    "            np.random.shuffle(new_ix)\n",
    "            data = data.reindex(index=new_ix)\n",
    "    \n",
    "    # deprecated, since I'm definitely going to use the modified dataset\n",
    "    else:\n",
    "        if end_dates:\n",
    "            data = pd.read_csv(\n",
    "                fname,\n",
    "                names=names,\n",
    "                index_col=0,\n",
    "                parse_dates=['start_date','end_date'])\n",
    "        else:\n",
    "            data = pd.read_csv(\n",
    "                fname,\n",
    "                names=names,\n",
    "                index_col=0,\n",
    "                parse_dates=['start_date'])\n",
    "            # shuffle indices\n",
    "            new_ix = list(data.index)\n",
    "            np.random.shuffle(new_ix)\n",
    "            data = data.reindex(index=new_ix)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory DA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = load_data('Taxi_Train_Routed.csv',shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data and subset for visual EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# don't remove 0-passenger outlier (not using passengers in model, so will leave in)\n",
    "# data = data[data['passengers'] != 0]\n",
    "\n",
    "# drop bad data\n",
    "data_clean = data\n",
    "# remove 0-fare data and implausibly low fares\n",
    "data_clean = data[data['fare'] > 3.5]\n",
    "# drop out-of-area low fare from Livermore area?\n",
    "data_clean = data_clean[data_clean['start_lo'] <= -122.197255]\n",
    "# no missing lat/long data (good!)\n",
    "\n",
    "# get index for the kept data\n",
    "ix_clean = data_clean.index\n",
    "\n",
    "\n",
    "\n",
    "# subset data for folium loading\n",
    "# subset = data.sample(n=5000,random_state=seed)\n",
    "# subset = data.iloc[10000:20000,:]\n",
    "# subset = data[data['start_la'] > (data.min()['start_la'] + (data.max()['start_la'] - data.min()['start_la'])*.7)]\n",
    "# subset = data[data['fare'] < 4]\n",
    "# coords = subset[['start_lo','start_la','end_lo','end_la','start_TAZ','end_TAZ']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, visualize the origin and destination, to see what region this dataset encompases. Look at potential bridge tolls, airport fees as contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use folium to map\n",
    "center = list(coords[['start_la','start_lo']].mean())\n",
    "\n",
    "def inline_map(map):\n",
    "    \"\"\"\n",
    "    Embeds the HTML source of the map directly into the IPython notebook.\n",
    "    \n",
    "    This method will not work if the map depends on any files (json data). Also this uses\n",
    "    the HTML5 srcdoc attribute, which may not be supported in all browsers.\n",
    "    \"\"\"\n",
    "    map._build_map()\n",
    "    return HTML('<iframe srcdoc=\"{srcdoc}\" style=\"width: 100%; height: 510px; border: none\"></iframe>'.format(srcdoc=map.HTML.replace('\"', '&quot;')))\n",
    "\n",
    "map_osm = folium.Map(location=center,zoom_start=9)\n",
    "for i in range(0,coords.shape[0]):\n",
    "    marker_data = coords.iloc[i,:]\n",
    "    map_osm.simple_marker([marker_data['start_la'],marker_data['start_lo']],popup='Start'+str(i))\n",
    "    map_osm.simple_marker([marker_data['end_la'],marker_data['end_lo']],popup='End'+str(i))\n",
    "inline_map(map_osm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: the majority of fares are within the city, but there are starting points outside of the city & at airports. Should include indicator for airport start, airport end, west/southbound bridge crossing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at relationship of n_passengers to fare - shouldn't be one..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.plot(kind='scatter',x='passengers',y='fare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it could have an effect, but I bet it will disappear when we take into account distance traveled. There was a 0-passenger data point, but I'm going to remove him (bad data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproccessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and append calculated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## TRAVEL TIME\n",
    "# convert to seconds since midnight\n",
    "to = (data['start_date'] - data['start_date'].apply(pd.datetools.normalize_date)).dt.seconds\n",
    "\n",
    "# compute tdelta\n",
    "tdelta = (data['end_date'] - data['start_date']).dt.seconds\n",
    "\n",
    "\n",
    "## TRAVEL DIST\n",
    "# convert to meters in UTM\n",
    "myProj = Proj(\"+proj=utm +zone=10 +ellps=WGS84 +datum=WGS84\")\n",
    "o_x,o_y = myProj(data['start_lo'].values,data['start_la'].values)\n",
    "d_x,d_y = myProj(data['end_lo'].values,data['end_la'].values)\n",
    "\n",
    "# compute l-2 and l-1 norm (l-1 potentially more useful as it describes driving distance if roads are all N-S...)\n",
    "d_l2 = np.sqrt((d_x-o_x)**2 + (d_y-o_y)**2)\n",
    "d_l1 = (d_x-o_x) + (d_y-o_y)\n",
    "\n",
    "\n",
    "## AIRPORT DEPARTURES AND ARRIVALS\n",
    "# TAZ info\n",
    "    # SFO: 239\n",
    "    # OAK: 895\n",
    "o_sfo,d_sfo,o_oak,d_oak = data['start_TAZ']==239, data['end_TAZ']==239, data['start_TAZ']==895, data['end_TAZ']==895\n",
    "\n",
    "\n",
    "## TOLL LIKELIHOOD\n",
    "# generate \"as the crow flies\" line denoting route\n",
    "route_line = gp.GeoSeries(\n",
    "    [shp.geometry.LineString([(o_lo,o_la),(d_lo,d_la)]) for o_lo,o_la,d_lo,d_la in zip(\n",
    "            data['start_lo'],\n",
    "            data['start_la'],\n",
    "            data['end_lo'],\n",
    "            data['end_la'])],\n",
    "    index=data.index)\n",
    "\n",
    "# create line separating east bay and SF\n",
    "dumbarton_bottom = [-122.06771850585938,37.46705828325743]\n",
    "dumbarton_top = [-122.16384887695312,37.54658486885501]\n",
    "sanMateo_top = [-122.28744506835938,37.69234455983933]\n",
    "bay_top = [-122.35954284667969,37.884439502831555]\n",
    "richmond_top = [-122.49886265167493,38.04134704836284]\n",
    "\n",
    "gg_E = [-122.420654296875,37.83131066825093]\n",
    "gg_W = [-122.94387817382812,37.68038998034691]\n",
    "\n",
    "dumbarton_sep = shp.geometry.LineString([dumbarton_bottom,dumbarton_top])\n",
    "sanMateo_sep = shp.geometry.LineString([dumbarton_top,sanMateo_top])\n",
    "bay_sep = shp.geometry.LineString([sanMateo_top,bay_top])\n",
    "richmond_sep = shp.geometry.LineString([bay_top,richmond_top])\n",
    "gg_sep = shp.geometry.LineString([gg_E,gg_W])\n",
    "\n",
    "# create indicators if route likely crossed each bridge going S/W\n",
    "toll_d = route_line.intersects(dumbarton_sep) & (data['start_lo'] > data['end_lo'])\n",
    "toll_s = route_line.intersects(sanMateo_sep) & (data['start_lo'] > data['end_lo'])\n",
    "toll_b = route_line.intersects(bay_sep) & (data['start_lo'] > data['end_lo'])\n",
    "toll_r = route_line.intersects(richmond_sep) & (data['start_lo'] > data['end_lo'])\n",
    "toll_g = route_line.intersects(gg_sep) & (data['start_la'] > data['end_la'])\n",
    "\n",
    "# only the bay bridge has a few toll payers (5), and a few of these look like they could have mistakes.\n",
    "# Including as a potentially useful feature\n",
    "toll = toll_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate full datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_full = pd.DataFrame({\n",
    "        'to':to,\n",
    "        'd_l1':d_l1,\n",
    "        'd_l2':d_l2,\n",
    "        'd_pg':data['distcost'],\n",
    "        'o_sfo':o_sfo,\n",
    "        'd_sfo':d_sfo,\n",
    "        'o_oak':o_oak,\n",
    "        'd_oak':d_oak,\n",
    "        'start_TAZ':data['start_TAZ'],\n",
    "        'end_TAZ':data['end_TAZ'],\n",
    "        'toll':toll,\n",
    "        'tdelta_pg':data['timecost'],\n",
    "        'passengers':data['passengers']\n",
    "    })\n",
    "X_full = X_full.reindex(columns=['to','d_l1','d_l2','d_pg','o_sfo',\n",
    "        'd_sfo','o_oak','d_oak','start_TAZ','end_TAZ','toll','tdelta_pg','passengers'])\n",
    "T = pd.Series(tdelta)\n",
    "Y = data['fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scalar = None\n",
    "\n",
    "# If we don't want all distance calculations included\n",
    "# time_cols = ['to','d_pg','o_sfo','d_sfo','o_oak','d_oak','start_TAZ','end_TAZ','toll','tdelta_pg']\n",
    "\n",
    "# include passengers? (doesn't seem to help)\n",
    "# time_cols = ['to','d_pg','o_sfo','d_sfo','o_oak','d_oak','start_TAZ','end_TAZ','toll','tdelta_pg','passengers']\n",
    "\n",
    "# settled on these for features to determine time of trip\n",
    "time_cols = ['to','d_pg','start_TAZ','end_TAZ','d_l1','tdelta_pg']\n",
    "# time_cols = ['to','start_TAZ'] # specifically to create profitability heat map by time of day\n",
    "\n",
    "# starting from basics for features to determine cost of trip\n",
    "fare_cols = ['to','d_pg','d_l1','tdelta_pred','start_TAZ','end_TAZ','tdelta_pg','toll']\n",
    "# fare_cols = ['to','start_TAZ'] # specifically to create profitability heat map by time of day\n",
    "\n",
    "X = X_full.reindex(columns=time_cols)\n",
    "X_fare = X_full.reindex(columns=fare_cols)\n",
    "\n",
    "# Try dummifying categoricals (didn't seem to help and way less efficient)\n",
    "# X = pd.get_dummies(X,columns=['start_TAZ','end_TAZ'])\n",
    "\n",
    "# get clean dataset for training\n",
    "X_clean = X.reindex(index=ix_clean)\n",
    "T_clean = T.reindex(index=ix_clean)\n",
    "Y_clean = Y.reindex(index=ix_clean)\n",
    "\n",
    "# scale? (unclear if it helps)\n",
    "# scalar = preprocessing.StandardScaler().fit(X_clean)\n",
    "# X = scalar.transform(X)\n",
    "# X_clean = scalar.transform(X_clean)\n",
    "\n",
    "# scale only the continuous vars?\n",
    "# non_cont_vars = X.reindex(columns=['o_sfo','d_sfo','o_oak','d_oak','start_TAZ','end_TAZ','toll'])\n",
    "# cont_vars = X.reindex(columns=['to','d_pg','tdelta_pg'])\n",
    "# scalar = preprocessing.StandardScaler().fit(cont_vars)\n",
    "# cont_vars_scaled = scalar.transform(cont_vars)\n",
    "# X = np.hstack((non_cont_vars.as_matrix(),cont_vars_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model (predicts overal mean fare for all data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE: 16.2152982839\n"
     ]
    }
   ],
   "source": [
    "mean_fare = Y.mean()\n",
    "rmse_baseline = np.sqrt(((Y - mean_fare)**2).mean())\n",
    "print 'Baseline RMSE:', rmse_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First develop a model for tdelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=13,\n",
       "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=1000, n_jobs=-1, oob_score=True, random_state=0,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random forest with pre-selected, default hyperparameters (using OOB score)\n",
    "TTmodel = RandomForestRegressor(n_estimators=1000,random_state=seed,oob_score=True,max_features='sqrt',n_jobs=-1,\n",
    "                               max_depth=13)\n",
    "TTmodel.fit(X_clean,T_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.677455988846\n",
      "[ 0.07906021  0.39059255  0.04862131  0.08174516  0.14549518  0.25448559]\n"
     ]
    }
   ],
   "source": [
    "# max_features = sqrt, max_depth=13 - 1000 trees\n",
    "print TTmodel.oob_score_\n",
    "print TTmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67375924674\n",
      "[ 0.0782447   0.4097142   0.04304123  0.08632155  0.14263487  0.24004345]\n"
     ]
    }
   ],
   "source": [
    "# max_features = sqrt, max_depth=13 - 100 trees\n",
    "print TTmodel.oob_score_\n",
    "print TTmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66768015207\n",
      "[ 0.11892483  0.33676027  0.05711293  0.08455217  0.17339237  0.22925744]\n"
     ]
    }
   ],
   "source": [
    "# max_features = sqrt - 1000 trees\n",
    "print TTmodel.oob_score_\n",
    "print TTmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.659322316194\n",
      "[ 0.11898633  0.35575389  0.05261577  0.08682394  0.1708041   0.21501597]\n"
     ]
    }
   ],
   "source": [
    "# max_features = sqrt - 100 trees\n",
    "print TTmodel.oob_score_\n",
    "print TTmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.652785044585\n",
      "[ 0.1205508   0.65564546  0.03888415  0.03996914  0.09286139  0.05208907]\n"
     ]
    }
   ],
   "source": [
    "# no max_features - 100 trees\n",
    "print TTmodel.oob_score_\n",
    "print TTmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.661042954435\n",
      "[ 0.12097615  0.65585835  0.03887621  0.04025422  0.09282464  0.05121043]\n"
     ]
    }
   ],
   "source": [
    "# basics + d_l1 (5000 trees) - **CURRENT WINNER**\n",
    "print TTmodel.oob_score_\n",
    "print TTmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.660515095521\n",
      "[ 0.12090801  0.65569451  0.03903179  0.04013464  0.09293356  0.05129749]\n"
     ]
    }
   ],
   "source": [
    "# basics + d_l1 (1000 trees)\n",
    "print TTmodel.oob_score_\n",
    "print TTmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.659882249817\n",
      "[ 0.12077517  0.65560884  0.03907315  0.04039182  0.09315811  0.05099291]\n"
     ]
    }
   ],
   "source": [
    "# basics + d_l1 (1000 trees) + shuffled\n",
    "print TTmodel.oob_score_\n",
    "print TTmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1137,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.526169439791\n",
      "[ 0.12236231  0.65527683  0.03999273  0.03936146  0.08926985  0.05373682]\n"
     ]
    }
   ],
   "source": [
    "# basics + d_l1 - *winner for small 10 trees*\n",
    "print TTmodel.oob_score_\n",
    "print TTmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.516586518816\n",
      "[ 0.12399812  0.65486359  0.03988887  0.03843279  0.09225773  0.05055892]\n"
     ]
    }
   ],
   "source": [
    "# basics + d_l1, shuffled\n",
    "print TTmodel.oob_score_\n",
    "print TTmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.513565862271\n",
      "[ 0.12890178  0.67600021  0.12214351  0.0729545 ]\n"
     ]
    }
   ],
   "source": [
    "# basics + d_l1 - TAZs\n",
    "print TTmodel.oob_score_\n",
    "print TTmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.523555323411\n",
      "[  1.74033767e-01   6.65108453e-01   4.80798008e-02   4.87803463e-02\n",
      "   2.94852208e-05   6.39681478e-02]\n"
     ]
    }
   ],
   "source": [
    "# adding toll (very small importance, and only affects a couple observations, so will leave out)\n",
    "print TTmodel.oob_score_\n",
    "print TTmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.522280117915\n",
      "[ 0.17383775  0.66568788  0.04773342  0.04890291  0.06383804]\n"
     ]
    }
   ],
   "source": [
    "# smallest set of features\n",
    "print TTmodel.oob_score_\n",
    "print TTmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tdelta_pred = TTmodel.predict(X)\n",
    "\n",
    "# Note the reindexing for the fare prediction IS NOT IMPLEMENTED FOR THE SCALED VERSION, SINCE I PRETTY \n",
    "# MUCH DECIDED NOT TO SCALE\n",
    "if scalar:\n",
    "    tdelta_pred_clean = TTmodel.predict(X_clean)\n",
    "    tdelta_array_clean = np.array([tdelta_pred_clean]).T\n",
    "    scalar_t = preprocessing.StandardScaler().fit(tdelta_array_clean)\n",
    "    tdelta_scaled_clean = scalar_t.transform(tdelta_array_clean)\n",
    "    X_clean = np.hstack((X_clean,tdelta_scaled_clean))\n",
    "    \n",
    "    tdelta_array = np.array([tdelta_pred]).T\n",
    "    tdelta_scaled = scalar_t.transform(tdelta_array)\n",
    "    X = np.hstack((X,tdelta_scaled))\n",
    "else:\n",
    "    X_fare['tdelta_pred']=tdelta_pred\n",
    "    X_fare = X_fare.reindex(columns=fare_cols)\n",
    "    # again,get clean dataset for training\n",
    "    X_fare_clean = X_fare.reindex(index=ix_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # random forest Grid Search for optimal parameters\n",
    "# tuned_params = {max_depth:[None,1,10,100]}\n",
    "# if model_type == 'RF':\n",
    "#             self.TTmodel = GridSearchCV(RandomForestRegressor,tuned_params, cv = cv, (random_state=0, n_estimators=50, max_depth=50)\n",
    "#             self.TTmodel.fit(self.X, self.T)\n",
    "#         else:\n",
    "#             print 'Unknown model type for travel times predictor.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now predict Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=16,\n",
       "           max_features='sqrt', max_leaf_nodes=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=1000, n_jobs=-1, oob_score=True, random_state=0,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random forest with pre-selected, default hyperparameters\n",
    "Fmodel = RandomForestRegressor(n_estimators=1000,random_state=seed,oob_score=True,n_jobs=-1,max_features='sqrt',\n",
    "                              max_depth=16)\n",
    "Fmodel.fit(X_fare_clean,Y_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93914638888\n",
      "[  1.26400654e-02   3.29755943e-01   8.92724087e-02   2.00364244e-01\n",
      "   4.91378621e-02   1.10184338e-01   2.08616262e-01   2.88764932e-05]\n"
     ]
    }
   ],
   "source": [
    "# max_features='sqrt', 1000 trees\n",
    "print Fmodel.oob_score_\n",
    "print Fmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.938174128449\n",
      "[  8.47620937e-03   8.77066835e-01   8.28598596e-03   8.79384607e-02\n",
      "   4.32116290e-03   6.90341377e-03   6.97227650e-03   3.56554372e-05]\n"
     ]
    }
   ],
   "source": [
    "# no max_features, 1000 trees, max_depth=16\n",
    "print Fmodel.oob_score_\n",
    "print Fmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.939529426031\n",
      "[  9.48921621e-03   3.31539350e-01   8.60255712e-02   2.02663556e-01\n",
      "   4.78727155e-02   1.11250928e-01   2.11130384e-01   2.82791998e-05]\n"
     ]
    }
   ],
   "source": [
    "# max_features=sqrt, 1000 trees, max_depth=16\n",
    "print Fmodel.oob_score_\n",
    "print Fmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.939280998697\n",
      "[  9.25154681e-03   2.86389990e-01   1.03238514e-01   2.13642194e-01\n",
      "   3.84773741e-02   1.04481650e-01   2.44486271e-01   3.24607624e-05]\n"
     ]
    }
   ],
   "source": [
    "# max_features=sqrt, 100 trees, max_depth=16\n",
    "print Fmodel.oob_score_\n",
    "print Fmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.937515628586\n",
      "[  1.30085533e-02   2.82253431e-01   1.08581899e-01   2.20012991e-01\n",
      "   3.98599430e-02   9.69819224e-02   2.39280311e-01   2.09502157e-05]\n"
     ]
    }
   ],
   "source": [
    "# max_features=sqrt, 100 trees\n",
    "print Fmodel.oob_score_\n",
    "print Fmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.937054538864\n",
      "[  1.09176336e-02   8.69072497e-01   1.03189111e-02   8.93683359e-02\n",
      "   4.95799011e-03   7.59829283e-03   7.72727515e-03   3.90639709e-05]\n"
     ]
    }
   ],
   "source": [
    "# no max_features, 100 trees\n",
    "print Fmodel.oob_score_\n",
    "print Fmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.947921110285\n",
      "[  1.12149493e-02   3.33812306e-01   8.88370890e-02   1.89909379e-01\n",
      "   4.86011525e-02   1.14297787e-01   2.13295586e-01   3.17510021e-05]\n"
     ]
    }
   ],
   "source": [
    "# same as below but unshuffled, max_depth = sqrt, and w/ 1000 trees\n",
    "print Fmodel.oob_score_\n",
    "print Fmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.949018959354\n",
      "[  8.75633125e-03   8.70256997e-01   8.61004138e-03   9.48068729e-02\n",
      "   3.91434876e-03   6.99165218e-03   6.61741859e-03   4.63375204e-05]\n"
     ]
    }
   ],
   "source": [
    "# same as below but unshuffled and w/ 5000 trees - **CURRENT WINNER**\n",
    "print Fmodel.oob_score_\n",
    "print Fmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1156,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.948905502461\n",
      "[  8.70733198e-03   8.70163175e-01   8.64677801e-03   9.48703342e-02\n",
      "   3.92550766e-03   7.00607524e-03   6.63835643e-03   4.24413604e-05]\n"
     ]
    }
   ],
   "source": [
    "# all features from time predition (basics + TAZs + PG time estimate + time of day at origin) + toll (1000 trees)\n",
    "print Fmodel.oob_score_\n",
    "print Fmodel.feature_importances_\n",
    "['to','d_pg','d_l1','tdelta_pred','start_TAZ','end_TAZ','tdelta_pg','toll']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.948620715911\n",
      "[  8.72657517e-03   8.70071812e-01   8.68699414e-03   9.48803931e-02\n",
      "   4.00916818e-03   6.82569163e-03   6.75465243e-03   4.47132532e-05]\n"
     ]
    }
   ],
   "source": [
    "# all features from time predition (basics + TAZs + PG time estimate + time of day at origin) + toll (1000 trees) + shuffled\n",
    "print Fmodel.oob_score_\n",
    "print Fmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1140,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.912797695171\n",
      "[  9.42865503e-03   8.70817062e-01   8.65432713e-03   9.28411175e-02\n",
      "   4.31332236e-03   7.37650385e-03   6.48898176e-03   8.00304669e-05]\n"
     ]
    }
   ],
   "source": [
    "# all features from time predition (basics + TAZs + PG time estimate + time of day at origin) + toll - winner for 10 trees\n",
    "print Fmodel.oob_score_\n",
    "print Fmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.908201216631\n",
      "[  9.91302155e-03   8.72775118e-01   8.39212058e-03   9.08358043e-02\n",
      "   3.95848892e-03   6.99082607e-03   7.07223572e-03   6.23844564e-05]\n"
     ]
    }
   ],
   "source": [
    "# all features from time predition (basics + TAZs + PG time estimate + time of day at origin) + toll + shuffled\n",
    "print Fmodel.oob_score_\n",
    "print Fmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.912034485627\n",
      "[  9.28821108e-03   8.70767731e-01   8.54633583e-03   9.28378256e-02\n",
      "   4.43100887e-03   6.86255277e-03   6.47258503e-03   8.54968117e-05\n",
      "   5.99911075e-04   1.07562704e-04   7.78807642e-07]\n"
     ]
    }
   ],
   "source": [
    "# all features from time predition + toll + airport flag\n",
    "print Fmodel.oob_score_\n",
    "print Fmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.910898216885\n",
      "[  9.41266609e-03   8.70729348e-01   8.73115523e-03   9.28173735e-02\n",
      "   4.10531820e-03   7.10523191e-03   6.26201635e-03   7.89886687e-05\n",
      "   6.42022135e-04   1.15879675e-04]\n"
     ]
    }
   ],
   "source": [
    "# all features from time predition + toll + SFO flag\n",
    "print Fmodel.oob_score_\n",
    "print Fmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.911474355796\n",
      "[ 0.00946347  0.87059879  0.0089815   0.09278085  0.00443954  0.007204\n",
      "  0.00653185]\n"
     ]
    }
   ],
   "source": [
    "# all features from time predition (basics + TAZs + PG time estimate + time of day at origin)\n",
    "print Fmodel.oob_score_\n",
    "print Fmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.908799860125\n",
      "[ 0.87190308  0.01170751  0.09525247  0.00544769  0.00825159  0.00743765]\n"
     ]
    }
   ],
   "source": [
    "# basics + TAZs + PG time estimate\n",
    "print Fmodel.oob_score_\n",
    "print Fmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.907454785598\n",
      "[  8.73960261e-01   1.29504106e-02   9.62861669e-02   6.26096143e-03\n",
      "   9.68845676e-03   7.01111570e-04   1.52631324e-04]\n"
     ]
    }
   ],
   "source": [
    "# basics + TAZs + SFO flags\n",
    "print Fmodel.oob_score_\n",
    "print Fmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.907880848168\n",
      "[  8.74006463e-01   1.30893750e-02   9.63481393e-02   6.17167293e-03\n",
      "   9.57361028e-03   6.95825152e-04   1.14566292e-04   3.47929747e-07]\n"
     ]
    }
   ],
   "source": [
    "# basics + TAZs + airport flags\n",
    "print Fmodel.oob_score_\n",
    "print Fmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.908222785837\n",
      "[ 0.87403146  0.01335816  0.09649309  0.00623581  0.00988148]\n"
     ]
    }
   ],
   "source": [
    "# basics + TAZs\n",
    "print Fmodel.oob_score_\n",
    "print Fmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.907804971987\n",
      "[  8.78457749e-01   1.99091840e-02   9.88086677e-02   2.46716881e-03\n",
      "   3.57230012e-04]\n"
     ]
    }
   ],
   "source": [
    "# basics + SFO flags only\n",
    "print Fmodel.oob_score_\n",
    "print Fmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.907816677117\n",
      "[  8.78322283e-01   1.99246051e-02   9.88041593e-02   2.70555152e-03\n",
      "   2.43399427e-04   0.00000000e+00   1.51737721e-09]\n"
     ]
    }
   ],
   "source": [
    "# basics + airport flags\n",
    "print Fmodel.oob_score_\n",
    "print Fmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.905754640994\n",
      "[  8.79307745e-01   2.14579277e-02   9.91397576e-02   9.45701450e-05]\n"
     ]
    }
   ],
   "source": [
    "# basics + toll\n",
    "print Fmodel.oob_score_\n",
    "print Fmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1028,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.906048630245\n",
      "[ 0.87940114  0.02132278  0.09927608]\n"
     ]
    }
   ],
   "source": [
    "# smallest set of fare features\n",
    "print Fmodel.oob_score_\n",
    "print Fmodel.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9580733541147968"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Fmodel.predict(X_fare)\n",
    "rmse = np.sqrt(((x-Y)**2).mean())\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try it on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess(fname,scalar=False,shuffle=True,\n",
    "                        time_columns=['to','d_pg','o_sfo','d_sfo','o_oak','d_oak',\n",
    "                                      'start_TAZ','end_TAZ','toll','tdelta_pg'],\n",
    "                       fare_columns=['to','d_pg','o_sfo','d_sfo','o_oak','d_oak',\n",
    "                                      'start_TAZ','end_TAZ','toll','tdelta_pg']):\n",
    "    \n",
    "    data = load_data(fname,end_dates=False,shuffle=shuffle)\n",
    "\n",
    "    \n",
    "    ## TRAVEL TIME\n",
    "    # convert to seconds since midnight\n",
    "    to = (data['start_date'] - data['start_date'].apply(pd.datetools.normalize_date)).dt.seconds\n",
    "    \n",
    "    ## TRAVEL DIST\n",
    "    # convert to meters in UTM\n",
    "    myProj = Proj(\"+proj=utm +zone=10 +ellps=WGS84 +datum=WGS84\")\n",
    "    o_x,o_y = myProj(data['start_lo'].values,data['start_la'].values)\n",
    "    d_x,d_y = myProj(data['end_lo'].values,data['end_la'].values)\n",
    "\n",
    "    # compute l-2 and l-1 norm (l-1 potentially more useful as it describes driving distance if roads are all N-S...)\n",
    "    d_l2 = np.sqrt((d_x-o_x)**2 + (d_y-o_y)**2)\n",
    "    d_l1 = (d_x-o_x) + (d_y-o_y)\n",
    "\n",
    "\n",
    "    ## AIRPORT DEPARTURES AND ARRIVALS\n",
    "    # TAZ info\n",
    "        # SFO: 239\n",
    "        # OAK: 895\n",
    "    o_sfo,d_sfo,o_oak,d_oak = data['start_TAZ']==239, data['end_TAZ']==239, data['start_TAZ']==895, data['end_TAZ']==895\n",
    "\n",
    "\n",
    "    ## TOLL LIKELIHOOD\n",
    "    # generate \"as the crow flies\" line denoting route\n",
    "    route_line = gp.GeoSeries(\n",
    "        [shp.geometry.LineString([(o_lo,o_la),(d_lo,d_la)]) for o_lo,o_la,d_lo,d_la in zip(\n",
    "                data['start_lo'],\n",
    "                data['start_la'],\n",
    "                data['end_lo'],\n",
    "                data['end_la'])],\n",
    "        index=data.index)\n",
    "\n",
    "    # create line separating east bay and SF\n",
    "    dumbarton_bottom = [-122.06771850585938,37.46705828325743]\n",
    "    dumbarton_top = [-122.16384887695312,37.54658486885501]\n",
    "    sanMateo_top = [-122.28744506835938,37.69234455983933]\n",
    "    bay_top = [-122.35954284667969,37.884439502831555]\n",
    "    richmond_top = [-122.49886265167493,38.04134704836284]\n",
    "\n",
    "    gg_E = [-122.420654296875,37.83131066825093]\n",
    "    gg_W = [-122.94387817382812,37.68038998034691]\n",
    "\n",
    "    dumbarton_sep = shp.geometry.LineString([dumbarton_bottom,dumbarton_top])\n",
    "    sanMateo_sep = shp.geometry.LineString([dumbarton_top,sanMateo_top])\n",
    "    bay_sep = shp.geometry.LineString([sanMateo_top,bay_top])\n",
    "    richmond_sep = shp.geometry.LineString([bay_top,richmond_top])\n",
    "    gg_sep = shp.geometry.LineString([gg_E,gg_W])\n",
    "\n",
    "    # create indicators if route likely crossed each bridge going S/W\n",
    "    toll_d = route_line.intersects(dumbarton_sep) & (data['start_lo'] > data['end_lo'])\n",
    "    toll_s = route_line.intersects(sanMateo_sep) & (data['start_lo'] > data['end_lo'])\n",
    "    toll_b = route_line.intersects(bay_sep) & (data['start_lo'] > data['end_lo'])\n",
    "    toll_r = route_line.intersects(richmond_sep) & (data['start_lo'] > data['end_lo'])\n",
    "    toll_g = route_line.intersects(gg_sep) & (data['start_la'] > data['end_la'])\n",
    "\n",
    "    # only the bay bridge has a few toll payers (5), and a few of these look like they could have mistakes.\n",
    "    # Including as a potentially useful feature\n",
    "    toll = toll_b\n",
    "\n",
    "    X = pd.DataFrame({\n",
    "        'to':to,\n",
    "        'd_l1':d_l1,\n",
    "        'd_l2':d_l2,\n",
    "        'd_pg':data['distcost'],\n",
    "        'o_sfo':o_sfo,\n",
    "        'd_sfo':d_sfo,\n",
    "        'o_oak':o_oak,\n",
    "        'd_oak':d_oak,\n",
    "        'start_TAZ':data['start_TAZ'],\n",
    "        'end_TAZ':data['end_TAZ'],\n",
    "        'toll':toll,\n",
    "        'tdelta_pg':data['timecost']\n",
    "    })\n",
    "    \n",
    "    X_fare = X.reindex(columns=fare_columns)\n",
    "    X = X.reindex(columns=time_columns)\n",
    "    \n",
    "    ix = X.index\n",
    "    \n",
    "    # scale if indicated\n",
    "    if scalar:\n",
    "        X = scalar.transform(X)\n",
    "    \n",
    "    return X,X_fare,ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test,X_fare_test,ix = load_and_preprocess('Taxi_Query_Routed.csv',\n",
    "                                            scalar=scalar,\n",
    "                                            time_columns=time_cols,\n",
    "                                            fare_columns=fare_cols,\n",
    "                                            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_test = TTmodel.predict(X_test)\n",
    "if scalar:\n",
    "    t_scaled = scalar_t.transform(t_test)\n",
    "    X_test = np.hstack((X_test,np.array([t_scaled]).T))\n",
    "else:\n",
    "    X_fare_test['tdelta_pred'] = t_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = Fmodel.predict(X_fare_test)\n",
    "y = pd.Series(y,index = ix, name='Fare').sort_index()\n",
    "y.to_csv('predictions.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10000.000000\n",
       "mean        17.526365\n",
       "std         16.249738\n",
       "min          5.384138\n",
       "25%          8.645272\n",
       "50%         11.404592\n",
       "75%         16.583473\n",
       "max        179.527540\n",
       "Name: Fare, dtype: float64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe() #6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10000.000000\n",
       "mean        17.588658\n",
       "std         16.391396\n",
       "min          4.629940\n",
       "25%          8.734614\n",
       "50%         11.454243\n",
       "75%         16.583563\n",
       "max        185.880888\n",
       "Name: Fare, dtype: float64"
      ]
     },
     "execution_count": 1196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe() #5d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10000.000000\n",
       "mean        17.588847\n",
       "std         16.404879\n",
       "min          4.586850\n",
       "25%          8.747837\n",
       "50%         11.432500\n",
       "75%         16.603928\n",
       "max        185.287540\n",
       "Name: Fare, dtype: float64"
      ]
     },
     "execution_count": 1178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe() #5c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10000.000000\n",
       "mean        17.586787\n",
       "std         16.389607\n",
       "min          4.637150\n",
       "25%          8.739400\n",
       "50%         11.445525\n",
       "75%         16.673723\n",
       "max        186.358050\n",
       "Name: Fare, dtype: float64"
      ]
     },
     "execution_count": 1162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe() #5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10000.00000\n",
       "mean        17.58540\n",
       "std         16.51138\n",
       "min          4.43500\n",
       "25%          8.69500\n",
       "50%         11.45500\n",
       "75%         16.64275\n",
       "max        189.52500\n",
       "Name: Fare, dtype: float64"
      ]
     },
     "execution_count": 1149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe() #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10000.000000\n",
       "mean        17.634176\n",
       "std         16.631098\n",
       "min          4.380000\n",
       "25%          8.670000\n",
       "50%         11.450000\n",
       "75%         16.975000\n",
       "max        192.155000\n",
       "Name: Fare, dtype: float64"
      ]
     },
     "execution_count": 990,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe() # 4c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10000.000000\n",
       "mean        17.612995\n",
       "std         16.564071\n",
       "min          4.435000\n",
       "25%          8.655000\n",
       "50%         11.482500\n",
       "75%         16.866500\n",
       "max        194.230000\n",
       "Name: Fare, dtype: float64"
      ]
     },
     "execution_count": 862,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe() # no scaling, with 0-passenger observation left in **CURRENT SUBMISSION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate heat map of profitability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TT_hm = TTmodel\n",
    "F_hm = Fmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>to</th>\n",
       "      <th>start_TAZ</th>\n",
       "      <th>t_pred</th>\n",
       "      <th>f_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>660</td>\n",
       "      <td>38</td>\n",
       "      <td>490.35</td>\n",
       "      <td>11.814958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1380</td>\n",
       "      <td>30</td>\n",
       "      <td>354.88</td>\n",
       "      <td>8.438467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2700</td>\n",
       "      <td>10</td>\n",
       "      <td>639.35</td>\n",
       "      <td>15.889667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2460</td>\n",
       "      <td>40</td>\n",
       "      <td>700.80</td>\n",
       "      <td>13.292750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4140</td>\n",
       "      <td>45</td>\n",
       "      <td>244.06</td>\n",
       "      <td>6.944375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      to  start_TAZ  t_pred     f_pred\n",
       "ID                                    \n",
       "0    660         38  490.35  11.814958\n",
       "1   1380         30  354.88   8.438467\n",
       "2   2700         10  639.35  15.889667\n",
       "3   2460         40  700.80  13.292750\n",
       "4   4140         45  244.06   6.944375"
      ]
     },
     "execution_count": 1328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_pred = TT_hm.predict(X_clean)\n",
    "f_pred = F_hm.predict(X_clean)\n",
    "\n",
    "X_hm = X_clean.copy()\n",
    "X_hm['t_pred'] = t_pred\n",
    "X_hm['f_pred'] = f_pred\n",
    "X_hm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(221, 3)"
      ]
     },
     "execution_count": 1329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get list of all TAZ's\n",
    "# predict for all TAZ's at 8 PM (to = 72000)\n",
    "TAZs = np.array([X_hm['start_TAZ'].unique()]).T\n",
    "n_TAZ = TAZs.shape[0]\n",
    "t = np.ones((n_TAZ,1))*72000\n",
    "to_pred = np.hstack((t,TAZs))\n",
    "to_pred.shape\n",
    "t_pred = np.array([TT_hm.predict(to_pred)]).T\n",
    "f_pred = np.array([F_hm.predict(to_pred)]).T\n",
    "results = np.hstack((TAZs,t_pred,f_pred))\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TAZ\n",
       "38    1.073801\n",
       "30    1.290888\n",
       "10    1.221343\n",
       "40    0.951611\n",
       "45    1.794105\n",
       "Name: profit_rate, dtype: float64"
      ]
     },
     "execution_count": 1330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create GeoDataFrame that has profit rate by TAZ\n",
    "hm_df = gp.GeoDataFrame(results,columns=['TAZ','travel_time','fare'])\n",
    "hm_df['profit_rate']=hm_df['fare'].div(hm_df['travel_time']) * 60 # $/min\n",
    "hm_df = hm_df.set_index('TAZ')\n",
    "pr_rate = hm_df['profit_rate']\n",
    "pr_rate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import shapefile of TAZs\n",
    "taz_shp = gp.GeoDataFrame.from_file('mtc_taz1454_clipurb2002/mtc_taz1454_clipurb2002.shp')\n",
    "taz_shp = taz_shp.set_index('TAZ1454').loc[:,['geometry']]\n",
    "\n",
    "# add profit_rate data (can't merge b/c loses geoDataFrame functionality)\n",
    "ix = taz_shp.index\n",
    "pr_rate = pr_rate.reindex(index=ix)\n",
    "taz_shp['profit_rate'] = pr_rate\n",
    "taz_shp = taz_shp[taz_shp['profit_rate'].notnull()]\n",
    "\n",
    "taz_shp_noid = taz_shp.copy()\n",
    "taz_shp_noid['TAZ'] = taz_shp.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Fiona:OGR Error 6: Normalized/laundered field name: 'profit_rate' to 'profit_rat'\n"
     ]
    }
   ],
   "source": [
    "taz_shp.to_file('pr_rate.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = taz_shp_noid.to_json()\n",
    "with open('data.json', 'w') as outfile:\n",
    "    json.dump(test, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'json_data' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1327-6d422055889a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmap_osm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeo_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeo_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data2.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TAZ'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'profit_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'feature.properties.TAZ'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0minline_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_osm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ianbolliger/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/folium/folium.pyc\u001b[0m in \u001b[0;36mgeo_json\u001b[0;34m(self, geo_path, data_out, data, columns, key_on, threshold_scale, fill_color, fill_opacity, line_color, line_weight, line_opacity, legend_name, topojson, reset)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;31m#Save data to JSON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_out\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0;31m#Add data to queue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ianbolliger/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/folium/utilities.pyc\u001b[0m in \u001b[0;36mtransform_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mjson_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mtype_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'json_data' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# use folium to map\n",
    "center = list(coords[['start_la','start_lo']].mean())\n",
    "\n",
    "def inline_map(map):\n",
    "    \"\"\"\n",
    "    Embeds the HTML source of the map directly into the IPython notebook.\n",
    "    \n",
    "    This method will not work if the map depends on any files (json data). Also this uses\n",
    "    the HTML5 srcdoc attribute, which may not be supported in all browsers.\n",
    "    \"\"\"\n",
    "    map._build_map()\n",
    "    return HTML('<iframe srcdoc=\"{srcdoc}\" style=\"width: 100%; height: 510px; border: none\"></iframe>'.format(srcdoc=map.HTML.replace('\"', '&quot;')))\n",
    "\n",
    "map_osm = folium.Map(location=center,zoom_start=9)\n",
    "# for i in range(0,coords.shape[0]):\n",
    "#     marker_data = coords.iloc[i,:]\n",
    "#     map_osm.simple_marker([marker_data['start_la'],marker_data['start_lo']],popup='Start'+str(i))\n",
    "#     map_osm.simple_marker([marker_data['end_la'],marker_data['end_lo']],popup='End'+str(i))\n",
    "# inline_map(map_osm)\n",
    "\n",
    "\n",
    "map_osm.geo_json(geo_path='data.json', data=gp, data_out='data2.json', columns=['TAZ','profit_rate'], key_on='feature.properties.TAZ')\n",
    "\n",
    "inline_map(map_osm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that I have a RF regressor with the available feature space paired down by CV, let's optimize hyperparameters\n",
    "\n",
    "### First, create model class that we can use to test various models via CV\n",
    "\n",
    "Then things to try:\n",
    " - grid search for best RF predictor\n",
    " - try other predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Travel Time predictor\n",
    "class TaxiFarePredictor:\n",
    "     \n",
    "    def __init__(self, X, T, Y, TTmodel_type = 'RF', Fmodel_type = 'RF'):\n",
    "        \n",
    "        \n",
    "        self.TTmodel_type = TTmodel_type # travel time model type\n",
    "        self.Fmodel_type = Fmodel_type # fare predictor model type\n",
    "        self.X, self.T, self.F = X, T, Y # features, travel time, fare\n",
    "    \n",
    "        # travel time model\n",
    "        self.travel_time_model(model_type = self.TTmodel_type)\n",
    "        \n",
    "        # train fare model\n",
    "        self.fare_model(model_type = self.Fmodel_type)\n",
    "    \n",
    "    \n",
    "    def travel_time_model(self, model_type='RF'):\n",
    "    \n",
    "        if model_type == 'RF':\n",
    "            self.TTmodel = RandomForestRegressor(random_state=0, n_estimators=50, max_depth=50)\n",
    "            self.TTmodel.fit(self.X, self.T)\n",
    "        else:\n",
    "            print 'Unknown model type for travel times predictor.'\n",
    "            \n",
    "    def grid_search(self, tuned_params, model_type='RF', cv=10):\n",
    "    \n",
    "        if model_type == 'RF':\n",
    "            self.TTmodel = GridSearchCV(RandomForestRegressor,tuned_params, cv = cv, (random_state=0, n_estimators=50, max_depth=50)\n",
    "            self.TTmodel.fit(self.X, self.T)\n",
    "        else:\n",
    "            print 'Unknown model type for travel times predictor.'\n",
    "            \n",
    "    \n",
    "    def travel_time(self, Xq):\n",
    "    \n",
    "        return self.TTmodel.predict(Xq)\n",
    "    \n",
    "    \n",
    "    def fare_model(self, model_type='RF'):\n",
    "        \n",
    "        T = self.T\n",
    "        X = self.X.join(T)\n",
    "        \n",
    "        if model_type == 'RF':\n",
    "            self.Fmodel = RandomForestRegressor(random_state=0, n_estimators=50, max_depth=50)\n",
    "            self.Fmodel.fit(X, self.F)  \n",
    "        else:\n",
    "            print 'Unknown model type for trip fare predictor.'\n",
    "    \n",
    "    \n",
    "    def fare(self, Xq):\n",
    "        \n",
    "        return self.Fmodel.predict(Xq)\n",
    "\n",
    "    \n",
    "    # can take (lat, lon) pairs or address as text, via geolocator geocoding\n",
    "    def TaxiFare(self, orig, dest, tstart='0:00', npax=1):\n",
    "        \n",
    "        if not isinstance(orig, basestring): \n",
    "            if not isinstance(dest, basestring): \n",
    "                \n",
    "                o = np.array(utm.from_latlon(orig[1], orig[0])[:2])\n",
    "                d = np.array(utm.from_latlon(dest[1], dest[0])[:2])\n",
    "               \n",
    "        else:\n",
    "            \n",
    "            try:\n",
    "                geolocator = Nominatim()\n",
    "\n",
    "                origin = geolocator.geocode(orig)\n",
    "                destination = geolocator.geocode(dest)\n",
    "    \n",
    "                o = np.array(utm.from_latlon(origin.latitude, origin.longitude)[:2])\n",
    "                d = np.array(utm.from_latlon(destination.latitude, destination.longitude)[:2])\n",
    "            \n",
    "            except:\n",
    "                \n",
    "                print 'Geocoding error. Try providing WGS84 coordinates instead.'\n",
    "                \n",
    "                return 0\n",
    "            \n",
    "        # euclidean distance between origin and destination\n",
    "        dist = cdist([o],[d])[0][0]\n",
    "    \n",
    "        #print str(depart_s)\n",
    "        temp = datetime.strptime('09/01/2012 '+ str(tstart), \"%m/%d/%Y %H:%M\")\n",
    "        depart = (temp - temp.replace(hour=0, minute=0, second=0, microsecond=0)).total_seconds()\n",
    "        \n",
    "        # features for travel time prediction\n",
    "        xq = np.append(np.array([float(npax), depart, dist]), np.append(o, d))\n",
    "                \n",
    "        # predict travel time\n",
    "        tt = self.travel_time(xq)\n",
    "        \n",
    "        # predict fare\n",
    "        f = self.fare(np.append(xq, tt))\n",
    "        \n",
    "        return f[0], tt[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results (RMSE scores):\n",
    "\n",
    "- full dataset\n",
    " - RF, no dummifying categoricals, no scaling of feature space, no grid-search: 1.8347498789313159\n",
    "   - saved as **predictions_draft3.csv**\n",
    " - Same as above + dummifying categoricals: 1.8481538826321522 (plus takes way longer, not clear that it's necessary but will try again when I reach a final model)\n",
    " - same as original, but including passengers: 1.8538311296987737 (also looks like it doesn't help)\n",
    " - same as original + scaling: 1.8449244519369137\n",
    "   - saved this as **predictions_draft2.csv**\n",
    "   - tried scaling with just continuous features, didn't make much diff. I think actually scaling everything makes more sense intuitively, will first have to OneHot the categorical TAZ features.\n",
    " - everything scaled, all data included: 1.9285044525152781\n",
    " - leaving in the 0-passenger data: 1.7324440652021653\n",
    "   - way better, strange, but giving it a shot **predictions_draft4.csv**\n",
    "   \n",
    "- started using the bad data in the test set, so rmse's go up\n",
    " - leaving in the 0-passenger data: 2.2713969401170373\n",
    "   - same as last test above, **predictions_draft4.csv**\n",
    " - leaving in all data: 1.9405090348346787\n",
    "   - makes it better, and with RF already using CV then maybe this is better - saving as **predictions_draft4b.csv**\n",
    " - same as draft4b, but with dummy vars for TAZ: 1.9690324617415753\n",
    " - same as draft4b, but with dummy vars for TAZ & scaling: 1.9802905995535145\n",
    "   - conclusion is that I will leave in categorical vars and not scale\n",
    "   \n",
    "*submitted both 4 and 4b, 4 did better, so will drop bad fare data from training from now on. current submission is* **predictions_draft4.csv**\n",
    "\n",
    "- dropping the airport flags: 2.2848759412016229\n",
    "    - **predictions_draft4c.csv**\n",
    "- optimizing feature space using OOB predictions in RF regressor: 2.2609827871916055\n",
    "    - **predictions_draft5.csv**\n",
    "- same as draft5, but using 1000 trees for forest, instead of 10: 2.0227200119801956\n",
    "    - **predictions_draft5b.csv** - **CURRENT WINNER**\n",
    "- same as draft5b, but shuffled. Score was a little worse, though RMSE a little better - probably insignificant: 2.0164465269603884\n",
    "    - **predictions_draft5c.csv** - also should submit with next round to compare to 5b\n",
    "- same as draft 5, but using 5000 trees: 2.0183348225744107\n",
    "    - **predictions_draft5d.csv**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
